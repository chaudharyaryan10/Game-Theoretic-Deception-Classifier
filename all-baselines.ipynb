{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":58310,"sourceType":"datasetVersion","datasetId":38310},{"sourceId":11140308,"sourceType":"datasetVersion","datasetId":6949070},{"sourceId":319088,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":269226,"modelId":290222},{"sourceId":319093,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":269231,"modelId":290227}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install allennlp==2.10.1 jsonlines numpydoc==0.9.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:14:40.164728Z","iopub.execute_input":"2025-04-03T09:14:40.165085Z","iopub.status.idle":"2025-04-03T09:18:39.166190Z","shell.execute_reply.started":"2025-04-03T09:14:40.165061Z","shell.execute_reply":"2025-04-03T09:18:39.165144Z"}},"outputs":[{"name":"stdout","text":"Collecting allennlp==2.10.1\n  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\nCollecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting numpydoc==0.9.2\n  Downloading numpydoc-0.9.2.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting torch<1.13.0,>=1.10.0 (from allennlp==2.10.1)\n  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\nCollecting torchvision<0.14.0,>=0.8.1 (from allennlp==2.10.1)\n  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\nCollecting cached-path<1.2.0,>=1.1.3 (from allennlp==2.10.1)\n  Downloading cached_path-1.1.6-py3-none-any.whl.metadata (6.0 kB)\nCollecting fairscale==0.4.6 (from allennlp==2.10.1)\n  Downloading fairscale-0.4.6.tar.gz (248 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting nltk>=3.6.5 (from allennlp==2.10.1)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting spacy<3.4,>=2.1.0 (from allennlp==2.10.1)\n  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (1.26.4)\nCollecting tensorboardX>=1.2 (from allennlp==2.10.1)\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (4.67.1)\nRequirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (3.12.1)\nRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (1.2.2)\nRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (1.13.1)\nRequirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (8.3.4)\nCollecting transformers<4.21,>=4.1 (from allennlp==2.10.1)\n  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (0.2.0)\nCollecting filelock<3.8,>=3.3 (from allennlp==2.10.1)\n  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting lmdb>=1.2.1 (from allennlp==2.10.1)\n  Downloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (10.5.0)\nCollecting termcolor==1.1.0 (from allennlp==2.10.1)\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting wandb<0.13.0,>=0.10.0 (from allennlp==2.10.1)\n  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (0.29.0)\nRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (0.3.8)\nCollecting base58>=2.1.1 (from allennlp==2.10.1)\n  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\nCollecting sacremoses (from allennlp==2.10.1)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (0.15.1)\nRequirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (3.20.3)\nRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.1) (5.7.1)\nCollecting jsonnet>=0.10.0 (from allennlp==2.10.1)\n  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from numpydoc==0.9.2) (8.1.3)\nRequirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from numpydoc==0.9.2) (3.1.4)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (25.1.0)\nCollecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1)\n  Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.36.23)\nRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (2.14.0)\nCollecting huggingface-hub>=0.0.16 (from allennlp==2.10.1)\n  Downloading huggingface_hub-0.10.1-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp==2.10.1) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp==2.10.1) (4.12.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp==2.10.1) (24.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->numpydoc==0.9.2) (3.0.2)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp==2.10.1) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp==2.10.1) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp==2.10.1) (2024.11.6)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.4->allennlp==2.10.1) (2.4.1)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.1) (1.2.2)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.1) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.1) (1.5.0)\nRequirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.1) (2.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.1) (2025.1.31)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp==2.10.1) (3.5.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (3.0.9)\nCollecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (0.7.11)\nCollecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (2.0.10)\nCollecting typer>=0.4.1 (from allennlp==2.10.1)\n  Downloading typer-0.4.2-py3-none-any.whl.metadata (12 kB)\nCollecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\nCollecting smart-open<7.0.0,>=5.2.1 (from spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\nCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (75.1.0)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp==2.10.1)\n  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1) (3.5.0)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.0.0)\nRequirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.19.1)\nRequirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (0.21.2)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (2.16.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.6.5->numpydoc==0.9.2) (1.4.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp==2.10.1) (11.0.0)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp==2.10.1)\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (3.1.43)\nRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (2.3)\nCollecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1)\n  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (5.9.5)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (2.19.2)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (1.17.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (0.4.0)\nCollecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1)\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (1.3.4)\nRequirement already satisfied: botocore<1.37.0,>=1.36.23 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.36.23)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (0.11.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (4.0.11)\nRequirement already satisfied: google-auth<3.0dev,>=2.23.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (2.27.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.34.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.6.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp==2.10.1) (1.3.0)\nCollecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp==2.10.1)\n  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\nCollecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1)\n  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.4->allennlp==2.10.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.4->allennlp==2.10.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.4->allennlp==2.10.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.4->allennlp==2.10.1) (2024.2.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.37.0,>=1.36.23->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (2.9.0.post0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.1) (5.0.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (1.66.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.4->allennlp==2.10.1) (2024.2.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp==2.10.1) (1.2.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1) (0.6.1)\nDownloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\nDownloading cached_path-1.1.6-py3-none-any.whl (26 kB)\nDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\nDownloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typer-0.4.2-py3-none-any.whl (27 kB)\nDownloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\nDownloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rich-12.6.0-py3-none-any.whl (237 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\nDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\nDownloading wasabi-0.10.1-py3-none-any.whl (26 kB)\nDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: numpydoc, fairscale, termcolor, jsonnet, pathtools\n  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for numpydoc: filename=numpydoc-0.9.2-py3-none-any.whl size=31890 sha256=efabe555a4d0bc33dbe8df88d2a8cad02836c7fd3fe1048773484298a700ff43\n  Stored in directory: /root/.cache/pip/wheels/f4/c4/8b/761765c1880e55650557755314fad04fa7ab4ec519632e648f\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307321 sha256=c04f65bbafaf0a393a930a58f23e07a7a2ff9b40b0796511e3ff820f5d39e324\n  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=ceabcbe3c27673004a29fe39390a638299fea22d99a7d1f052c3ce7601290974\n  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406878 sha256=ffc80330b57fb880984d66ddf7c5cd4f6174e19332afea37aadf6dad4b41e0db\n  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=41f5f15b57fc8c084c1fcf47e5f189d5e530ddc28b061b6bcd51dbf6cd0b41b0\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\nSuccessfully built numpydoc fairscale termcolor jsonnet pathtools\nInstalling collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, typing-extensions, typer, smart-open, shortuuid, sacremoses, rich, pathlib-abc, nltk, jsonlines, filelock, base58, torch, pydantic, pathy, huggingface-hub, wandb, numpydoc, fairscale, cached-path, thinc, transformers, torchvision, tensorboardX, spacy, allennlp\n  Attempting uninstall: wasabi\n    Found existing installation: wasabi 1.1.3\n    Uninstalling wasabi-1.1.3:\n      Successfully uninstalled wasabi-1.1.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.5.0\n    Uninstalling termcolor-2.5.0:\n      Successfully uninstalled termcolor-2.5.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Uninstalling typing_extensions-4.12.2:\n      Successfully uninstalled typing_extensions-4.12.2\n  Attempting uninstall: typer\n    Found existing installation: typer 0.15.1\n    Uninstalling typer-0.15.1:\n      Successfully uninstalled typer-0.15.1\n  Attempting uninstall: smart-open\n    Found existing installation: smart-open 7.0.5\n    Uninstalling smart-open-7.0.5:\n      Successfully uninstalled smart-open-7.0.5\n  Attempting uninstall: rich\n    Found existing installation: rich 13.9.4\n    Uninstalling rich-13.9.4:\n      Successfully uninstalled rich-13.9.4\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.17.0\n    Uninstalling filelock-3.17.0:\n      Successfully uninstalled filelock-3.17.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.0a2\n    Uninstalling pydantic-2.11.0a2:\n      Successfully uninstalled pydantic-2.11.0a2\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.29.0\n    Uninstalling huggingface-hub-0.29.0:\n      Successfully uninstalled huggingface-hub-0.29.0\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.1\n    Uninstalling wandb-0.19.1:\n      Successfully uninstalled wandb-0.19.1\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.2.5\n    Uninstalling thinc-8.2.5:\n      Successfully uninstalled thinc-8.2.5\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.7.5\n    Uninstalling spacy-3.7.5:\n      Successfully uninstalled spacy-3.7.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsqlalchemy 2.0.36 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\naccelerate 1.2.1 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.10.1 which is incompatible.\nalbumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.8.2 which is incompatible.\naltair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\ndatasets 3.3.1 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.10.1 which is incompatible.\ndiffusers 0.31.0 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.10.1 which is incompatible.\nen-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\ngoogle-genai 0.2.2 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.8.2 which is incompatible.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.20.1 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nlangchain 0.3.12 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.8.2 which is incompatible.\nlangchain-core 0.3.25 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.8.2 which is incompatible.\nlangchain-core 0.3.25 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\nnibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\nopenai 1.57.4 requires pydantic<3,>=1.9.0, but you have pydantic 1.8.2 which is incompatible.\nopenai 1.57.4 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\npeft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.10.1 which is incompatible.\npeft 0.14.0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\npyopenssl 25.0.0 requires typing-extensions>=4.9; python_version < \"3.13\" and python_version >= \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\npydantic-core 2.29.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\npymc 5.19.1 requires rich>=13.7.1, but you have rich 12.6.0 which is incompatible.\npytensor 2.26.4 requires filelock>=3.15, but you have filelock 3.7.1 which is incompatible.\npytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 1.12.1 which is incompatible.\nsentence-transformers 3.3.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.10.1 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.20.1 which is incompatible.\nsigstore 3.6.1 requires pydantic<3,>=2, but you have pydantic 1.8.2 which is incompatible.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 12.6.0 which is incompatible.\nsigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.8.2 which is incompatible.\nstable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.12.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.12.1 which is incompatible.\ntorchmetrics 1.6.1 requires torch>=2.0.0, but you have torch 1.12.1 which is incompatible.\ntypeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\nydata-profiling 4.12.2 requires pydantic>=2, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed allennlp-2.10.1 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 fairscale-0.4.6 filelock-3.7.1 huggingface-hub-0.10.1 jsonlines-4.0.0 jsonnet-0.20.0 lmdb-1.6.2 nltk-3.9.1 numpydoc-0.9.2 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pydantic-1.8.2 rich-12.6.0 sacremoses-0.1.1 shortuuid-1.0.13 smart-open-6.4.0 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/DenisPeskoff/2020_acl_diplomacy.git\n%cd 2020_acl_diplomacy\n!python utils/singlemessage_format.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:18:44.859353Z","iopub.execute_input":"2025-04-03T09:18:44.859693Z","iopub.status.idle":"2025-04-03T09:18:46.441631Z","shell.execute_reply.started":"2025-04-03T09:18:44.859663Z","shell.execute_reply":"2025-04-03T09:18:46.440579Z"}},"outputs":[{"name":"stdout","text":"Cloning into '2020_acl_diplomacy'...\nremote: Enumerating objects: 682, done.\u001b[K\nremote: Counting objects: 100% (46/46), done.\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 682 (delta 30), reused 26 (delta 26), pack-reused 636 (from 1)\u001b[K\nReceiving objects: 100% (682/682), 1.52 MiB | 10.30 MiB/s, done.\nResolving deltas: 100% (504/504), done.\n/kaggle/working/2020_acl_diplomacy\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE HUMAN BASELINE : \")\n!python diplomacy/models/human_baseline.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:55:47.323413Z","iopub.execute_input":"2025-04-03T09:55:47.323806Z","iopub.status.idle":"2025-04-03T09:55:48.237870Z","shell.execute_reply.started":"2025-04-03T09:55:47.323776Z","shell.execute_reply":"2025-04-03T09:55:48.236805Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE HUMAN BASELINE : \nHuman baseline, macro: 0.5814484420580899\nHuman baseline, lie F1: 0.22580645161290322\nOverall Accuracy is,  0.8836363636363637\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE RANDOM AND MAJORITY BASELINE : \")\n!python diplomacy/models/random_and_majority_baselines.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:57:11.949795Z","iopub.execute_input":"2025-04-03T09:57:11.950209Z","iopub.status.idle":"2025-04-03T09:57:30.131691Z","shell.execute_reply.started":"2025-04-03T09:57:11.950175Z","shell.execute_reply":"2025-04-03T09:57:30.130573Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE RANDOM AND MAJORITY BASELINE : \nTotal sender samples: 2741\nTotal receiver samples: 2475\nmacro\nSender Random F1 0.3971131520056381\nReceiver Random F1 0.38430776546215645\nSender Majority Class F1 0.4771079740557069\nReceiver Majority Class F1 0.4827586206896527\nbinary\nSender Random F1 0.14864505621604604\nReceiver Random F1 0.11817088555333304\nSender Majority Class F1 0.0\nReceiver Majority Class F1 0.0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE HARBRINGERS BASELINE (ACTUAL LIE): \")\n!python diplomacy/models/harbringers.py s n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:58:16.950949Z","iopub.execute_input":"2025-04-03T09:58:16.951349Z","iopub.status.idle":"2025-04-03T09:59:14.011141Z","shell.execute_reply.started":"2025-04-03T09:58:16.951315Z","shell.execute_reply":"2025-04-03T09:59:14.010196Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE HARBRINGERS BASELINE (ACTUAL LIE): \n2025-04-03 09:58:19.004401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 09:58:19.026382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 09:58:19.033187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.157     0.562     0.246       240\n           1      0.944     0.711     0.811      2501\n\n    accuracy                          0.698      2741\n   macro avg      0.551     0.637     0.528      2741\nweighted avg      0.875     0.698     0.761      2741\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE HARBRINGERS WITH POWER BASELINE (ACTUAL LIE): \")\n!python diplomacy/models/harbringers.py s y  # without power","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:38:00.082048Z","iopub.execute_input":"2025-03-25T13:38:00.082589Z","iopub.status.idle":"2025-03-25T13:39:09.888944Z","shell.execute_reply.started":"2025-03-25T13:38:00.082550Z","shell.execute_reply":"2025-03-25T13:39:09.887582Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE HARBRINGERS WITH POWER BASELINE (ACTUAL LIE): \n2025-03-25 13:38:02.315463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-25 13:38:02.342786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-25 13:38:02.350935: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.154     0.517     0.237       240\n           1      0.940     0.728     0.820      2501\n\n    accuracy                          0.709      2741\n   macro avg      0.547     0.622     0.529      2741\nweighted avg      0.871     0.709     0.769      2741\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE HARBRINGERS BASELINE (SUSPECTED LIE): \")\n!python diplomacy/models/harbringers.py r n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:39:13.122526Z","iopub.execute_input":"2025-03-25T13:39:13.122934Z","iopub.status.idle":"2025-03-25T13:40:19.252403Z","shell.execute_reply.started":"2025-03-25T13:39:13.122898Z","shell.execute_reply":"2025-03-25T13:40:19.250663Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE HARBRINGERS BASELINE (SUSPECTED LIE): \n2025-03-25 13:39:15.258522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-25 13:39:15.287028: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-25 13:39:15.295773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.087     0.467     0.147       165\n           1      0.945     0.651     0.771      2310\n\n    accuracy                          0.638      2475\n   macro avg      0.516     0.559     0.459      2475\nweighted avg      0.888     0.638     0.729      2475\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE HARBRINGERS WITH POWER BASELINE (SUSPECTED LIE): \")\n!python diplomacy/models/harbringers.py r y  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:40:37.765881Z","iopub.execute_input":"2025-03-25T13:40:37.766558Z","iopub.status.idle":"2025-03-25T13:41:44.637916Z","shell.execute_reply.started":"2025-03-25T13:40:37.766514Z","shell.execute_reply":"2025-03-25T13:41:44.636408Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE HARBRINGERS WITH POWER BASELINE (SUSPECTED LIE): \n2025-03-25 13:40:40.229425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-25 13:40:40.262763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-25 13:40:40.272179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.091     0.533     0.155       165\n           1      0.949     0.617     0.748      2310\n\n    accuracy                          0.612      2475\n   macro avg      0.520     0.575     0.451      2475\nweighted avg      0.892     0.612     0.708      2475\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"with open(\"diplomacy/models/bagofwords.py\", \"r\") as f:\n    lines = f.readlines()\n\nnew_lines = []\nfor line in lines:\n    if \"stop_words=STOP_WORDS\" in line:\n        new_lines.append(line.replace(\"stop_words=STOP_WORDS\", \"stop_words='english'\"))\n    else:\n        new_lines.append(line)\n\nwith open(\"diplomacy/models/bagofwords.py\", \"w\") as f:\n    f.writelines(new_lines)\n    \n!python utils/singlemessage_format.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:23:48.524448Z","iopub.execute_input":"2025-04-03T09:23:48.524933Z","iopub.status.idle":"2025-04-03T09:23:48.848666Z","shell.execute_reply.started":"2025-04-03T09:23:48.524890Z","shell.execute_reply":"2025-04-03T09:23:48.847472Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE BAG-OF-WORDS BASELINE (ACTUAL LIE): \")\n!python diplomacy/models/bagofwords.py s n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:59:34.020044Z","iopub.execute_input":"2025-04-03T09:59:34.020434Z","iopub.status.idle":"2025-04-03T09:59:44.735969Z","shell.execute_reply.started":"2025-04-03T09:59:34.020402Z","shell.execute_reply":"2025-04-03T09:59:44.735097Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE BAG-OF-WORDS BASELINE (ACTUAL LIE): \n2025-04-03 09:59:36.091710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 09:59:36.113179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 09:59:36.119768: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.149     0.225     0.179       240\n           1      0.922     0.876     0.899      2501\n\n    accuracy                          0.819      2741\n   macro avg      0.535     0.551     0.539      2741\nweighted avg      0.854     0.819     0.836      2741\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE BAG-OF-WORDS WITH POWER BASELINE (ACTUAL LIE): \")\n!python diplomacy/models/bagofwords.py s y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:24:32.837063Z","iopub.execute_input":"2025-04-03T09:24:32.837384Z","iopub.status.idle":"2025-04-03T09:24:43.380729Z","shell.execute_reply.started":"2025-04-03T09:24:32.837358Z","shell.execute_reply":"2025-04-03T09:24:43.379636Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE BAG-OF-WORDS WITH POWER BASELINE (ACTUAL LIE): \n2025-04-03 09:24:34.830477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 09:24:34.853685: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 09:24:34.860382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.162     0.254     0.198       240\n           1      0.924     0.874     0.898      2501\n\n    accuracy                          0.819      2741\n   macro avg      0.543     0.564     0.548      2741\nweighted avg      0.858     0.819     0.837      2741\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE BAG-OF-WORDS BASELINE (SUSPECTED LIE): \")\n!python diplomacy/models/bagofwords.py r n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T13:51:23.317108Z","iopub.execute_input":"2025-03-25T13:51:23.317537Z","iopub.status.idle":"2025-03-25T13:51:36.772618Z","shell.execute_reply.started":"2025-03-25T13:51:23.317499Z","shell.execute_reply":"2025-03-25T13:51:36.771078Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE BAG-OF-WORDS BASELINE (SUSPECTED LIE): \n2025-03-25 13:51:25.567780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-25 13:51:25.596881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-25 13:51:25.605539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.102     0.236     0.143       165\n           1      0.940     0.852     0.894      2310\n\n    accuracy                          0.811      2475\n   macro avg      0.521     0.544     0.518      2475\nweighted avg      0.884     0.811     0.844      2475\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print('='*75)\nprint(\"RESULTS OF THE BAG-OF-WORDS WITH POWER BASELINE (SUSPECTED LIE): \")\n!python diplomacy/models/bagofwords.py r y  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:41:37.517773Z","iopub.execute_input":"2025-04-03T09:41:37.518156Z","iopub.status.idle":"2025-04-03T09:41:48.718845Z","shell.execute_reply.started":"2025-04-03T09:41:37.518132Z","shell.execute_reply":"2025-04-03T09:41:48.717782Z"}},"outputs":[{"name":"stdout","text":"===========================================================================\nRESULTS OF THE BAG-OF-WORDS WITH POWER BASELINE (SUSPECTED LIE): \n2025-04-03 09:41:40.129474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-03 09:41:40.151996: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-03 09:41:40.158688: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n              precision    recall  f1-score   support\n\n           0      0.103     0.242     0.144       165\n           1      0.940     0.849     0.892      2310\n\n    accuracy                          0.808      2475\n   macro avg      0.521     0.546     0.518      2475\nweighted avg      0.884     0.808     0.842      2475\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"IMPORTS ","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nimport numpy as np\nimport torch\nfrom collections import defaultdict\nimport math\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score, precision_score, recall_score , confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:25:01.233894Z","iopub.execute_input":"2025-04-03T09:25:01.234262Z","iopub.status.idle":"2025-04-03T09:25:02.688355Z","shell.execute_reply.started":"2025-04-03T09:25:01.234230Z","shell.execute_reply":"2025-04-03T09:25:02.687593Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"SEED = 1994\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:26:05.485348Z","iopub.execute_input":"2025-04-03T09:26:05.486029Z","iopub.status.idle":"2025-04-03T09:26:05.535124Z","shell.execute_reply.started":"2025-04-03T09:26:05.486001Z","shell.execute_reply":"2025-04-03T09:26:05.534444Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"embeddings_path = \"/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt\"\ntrain_path = \"/kaggle/input/deception/train.jsonl\"\nval_path = \"/kaggle/input/deception/validation.jsonl\"\ntest_path = \"/kaggle/input/deception/test.jsonl\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:26:09.096351Z","iopub.execute_input":"2025-04-03T09:26:09.096699Z","iopub.status.idle":"2025-04-03T09:26:09.100376Z","shell.execute_reply.started":"2025-04-03T09:26:09.096671Z","shell.execute_reply":"2025-04-03T09:26:09.099529Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"embeddings = [np.zeros(200, dtype=np.float32)] ; embeddings.append(np.random.normal(scale=0.06, size=(200,)).astype(np.float32))\n\nw2i = {} ; w2i[\"<PAD>\"] = 0 ; w2i[\"<UNK>\"] = 1\n\n\ni2w = {} ; i2w[0] = \"<PAD>\" ; i2w[1] = \"<UNK>\"\n\n\nptr = len(embeddings)  \nwith open(embeddings_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split()\n        if len(parts) != 201:\n            continue  \n        w = parts[0]\n        vec = np.asarray(parts[1:], dtype=np.float32)\n        w2i[w] = ptr\n        i2w[ptr] = w\n        ptr += 1\n        embeddings.append(vec)\n\nembedding_mat = torch.FloatTensor(np.array(embeddings))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:26:28.042532Z","iopub.execute_input":"2025-04-03T09:26:28.042845Z","iopub.status.idle":"2025-04-03T09:27:29.403061Z","shell.execute_reply.started":"2025-04-03T09:26:28.042824Z","shell.execute_reply":"2025-04-03T09:27:29.402277Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def tokenize(text):\n    return text.lower().strip().split()\n\ndef text_to_ids(text, w2i, unk_token=\"<UNK>\"):\n    tokens = tokenize(text)\n    ids = []\n    for t in tokens:\n        if t in w2i:\n            ids.append(w2i[t])\n        else:\n            ids.append(w2i.get(unk_token, 0))\n    return ids\n\nclass CustomDataset(Dataset):\n    def __init__(self, jsonl_path):\n        super().__init__()\n        self.samples = []\n        df = pd.read_json(jsonl_path, lines=True)\n        for index, row in df.iterrows():\n            messages = row[\"messages\"]\n            labels = row[\"sender_labels\"]  \n            for text, label in zip(messages, labels):\n                text_ids = text_to_ids(text, w2i)\n                self.samples.append((text_ids, label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\ndef collate_fn(batch):\n    max_len = max(len(sample[0]) for sample in batch)\n    padded_texts = []\n    labels = []\n    for text_ids, label in batch:\n        padded = text_ids + [0] * (max_len - len(text_ids))  \n        padded_texts.append(padded)\n        labels.append(label)\n\n    padded_texts = torch.LongTensor(padded_texts)\n    labels = torch.LongTensor(labels)\n    return padded_texts, labels\n\ntrain_dataset = CustomDataset(train_path)\nval_dataset = CustomDataset(val_path)\ntest_dataset = CustomDataset(test_path)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:27:36.536580Z","iopub.execute_input":"2025-04-03T09:27:36.536925Z","iopub.status.idle":"2025-04-03T09:27:37.035224Z","shell.execute_reply.started":"2025-04-03T09:27:36.536897Z","shell.execute_reply":"2025-04-03T09:27:37.034581Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# we have taken ref for thepytorch implementation of lie_detector.py as per git repo \nclass LieDetector(nn.Module):\n    def __init__(self,embedding_matrix,hidden_size,use_power,posclass_weight,dropout,num_classes):\n        super().__init__()\n\n        self.use_power = use_power\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n        emb_dim = embedding_matrix.shape[1]\n\n        self.encoder = nn.LSTM(emb_dim, hidden_size, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n\n        input_dim = 2 * hidden_size + (1 if use_power else 0)\n        self.classifier = nn.Linear(input_dim, num_classes)\n\n        class_weights = torch.tensor([posclass_weight, 1.0], dtype=torch.float)\n        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\n        self.reset_metrics()\n\n    def reset_metrics(self):\n        self.true_y = []\n        self.pred_y = []\n\n    def compute_metrics(self):\n        if len(self.true_y) == 0:\n            return {}\n\n        y_true = torch.cat(self.true_y).cpu().numpy()\n        y_pred = torch.cat(self.pred_y).cpu().numpy()\n\n        return {\n            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n            \"truth_precision\": precision_score(y_true, y_pred, pos_label=1),\n            \"truth_recall\": recall_score(y_true, y_pred, pos_label=1),\n            \"truth_fscore\": f1_score(y_true, y_pred, pos_label=1),\n            \"lie_precision\": precision_score(y_true, y_pred, pos_label=0),\n            \"lie_recall\": recall_score(y_true, y_pred, pos_label=0),\n            \"lie_fscore\": f1_score(y_true, y_pred, pos_label=0), \n        }\n\n\n    def forward(self, input_ids, score_delta=None, labels=None):\n        mask = input_ids != 0\n        lengths = mask.sum(dim=1).cpu()\n\n        embedded = self.embedding(input_ids)\n        \n        if not (lengths == lengths[0]).all():\n            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n            packed_output, _ = self.encoder(packed_embedded)\n            encoded_seq, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n        else:\n            encoded_seq, _ = self.encoder(embedded)\n\n        output_dim = encoded_seq.size(-1)\n        seq_mask = mask.unsqueeze(-1).expand(-1, -1, output_dim)\n        encoded_seq = encoded_seq.masked_fill(~seq_mask, float('-inf'))\n        pooled = torch.max(encoded_seq, dim=1)[0]\n\n        if self.use_power and score_delta is not None:\n            pooled = torch.cat([pooled, score_delta.unsqueeze(1)], dim=1)\n\n        pooled = self.dropout(pooled)\n        logits = self.classifier(pooled)\n\n        output = {\"logits\": logits}\n\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n            output[\"loss\"] = loss\n            preds = torch.argmax(logits, dim=1)\n            self.true_y.append(labels.detach())\n            self.pred_y.append(preds.detach())\n        return output\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = LieDetector(\n    embedding_matrix=embedding_mat,\n    hidden_size=100,\n    use_power=False,\n    posclass_weight=30.0,\n    dropout=0.5,\n    num_classes=2\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:27:46.784225Z","iopub.execute_input":"2025-04-03T09:27:46.784580Z","iopub.status.idle":"2025-04-03T09:27:50.962876Z","shell.execute_reply.started":"2025-04-03T09:27:46.784548Z","shell.execute_reply":"2025-04-03T09:27:50.962194Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.003)  # Matches config\nf1_b = -1\nct = 0\n\nfrom sklearn.metrics import accuracy_score\n\ndef evaluate(model, data_loader):\n    model.eval()\n    model.reset_metrics()\n    tot = 0.0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for x, y in data_loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            output = model(x, labels=y)\n            logits = output[\"logits\"]\n            loss = output[\"loss\"]\n\n            tot += loss.item() * y.size(0)\n\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.cpu().numpy())\n\n    av = tot / len(data_loader.dataset)\n    metrics = model.compute_metrics()\n    acc = accuracy_score(all_labels, all_preds)\n    metrics[\"accuracy\"] = acc\n\n    for k, v in metrics.items():\n        print(k, \" \", v)\n\n    return av, metrics[\"macro_f1\"], metrics[\"lie_fscore\"], metrics[\"accuracy\"]\n\n\nfor epoch in range(15):\n    model.train()\n    ttl = 0.0\n    tp = []\n    t_gt = []\n\n    for x, y in train_loader:\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n        output = model(x, labels=y)\n        logits = output[\"logits\"]\n        loss = output[\"loss\"]\n\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        ttl += loss.item()*y.size(0)\n        \n        preds = torch.argmax(logits, dim=1)\n        tp.extend(preds.cpu().numpy())\n        t_gt.extend(y.cpu().numpy())\n\n    tr_f1 = f1_score(t_gt, tp, average=\"macro\")\n    av_tl = ttl / len(train_loader.dataset)\n    \n    vl, val_f1, lie_f1 , acc = evaluate(model, val_loader)\n\n    print(\"Epoch \" + str(epoch + 1) + \" Train Loss: \" + str(round(av_tl, 4)) + \"  Train F1: \" + str(round(tr_f1, 4)) + \"  Val Loss: \" + str(round(vl, 4)) + \"  Val Macro-F1: \" + str(round(val_f1, 4)) + \"  Val Lie-F1: \" + str(round(lie_f1, 4)))\n\n    if val_f1 > f1_b:\n        f1_b = val_f1\n        ct = 0\n        best_model_state = {\n            \"epoch\": epoch + 1,\n            \"model_state\": model.state_dict(),\n            \"val_f1\": val_f1\n        }\n        print(\"New best model saved with Val F1: {val_f1:.4f}\")\n    else:\n        ct += 1\n        print(\"No improvement for \",ct,\"epochs\")\n        if ct >= 5:\n            print(\"Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:34:18.979770Z","iopub.execute_input":"2025-03-25T17:34:18.980163Z","iopub.status.idle":"2025-03-25T17:34:49.104852Z","shell.execute_reply.started":"2025-03-25T17:34:18.980131Z","shell.execute_reply":"2025-03-25T17:34:49.104025Z"}},"outputs":[{"name":"stdout","text":"macro_f1   0.5241005760377668\nmicro_f1   0.8785310734463276\ntruth_precision   0.9647887323943662\ntruth_recall   0.9066176470588235\ntruth_fscore   0.9347990902198635\nlie_precision   0.07971014492753623\nlie_recall   0.19642857142857142\nlie_fscore   0.11340206185567009\naccuracy   0.8785310734463276\nEpoch 1 Train Loss: 0.7172  Train F1: 0.4271  Val Loss: 0.6396  Val Macro-F1: 0.5241  Val Lie-F1: 0.1134\nNew best model saved with Val F1: {val_f1:.4f}\nmacro_f1   0.5069779254240621\nmicro_f1   0.9399717514124294\ntruth_precision   0.9609544468546638\ntruth_recall   0.9772058823529411\ntruth_fscore   0.969012030623405\nlie_precision   0.06060606060606061\nlie_recall   0.03571428571428571\nlie_fscore   0.0449438202247191\naccuracy   0.9399717514124294\nEpoch 2 Train Loss: 0.6814  Train F1: 0.4433  Val Loss: 0.6706  Val Macro-F1: 0.507  Val Lie-F1: 0.0449\nNo improvement for  1 epochs\nmacro_f1   0.36947018961407446\nmicro_f1   0.4759887005649718\ntruth_precision   0.9843260188087775\ntruth_recall   0.46176470588235297\ntruth_fscore   0.6286286286286286\nlie_precision   0.05912596401028278\nlie_recall   0.8214285714285714\nlie_fscore   0.11031175059952038\naccuracy   0.4759887005649718\nEpoch 3 Train Loss: 0.6642  Train F1: 0.4852  Val Loss: 0.7063  Val Macro-F1: 0.3695  Val Lie-F1: 0.1103\nNo improvement for  2 epochs\nmacro_f1   0.5063477270373822\nmicro_f1   0.8573446327683616\ntruth_precision   0.9632\ntruth_recall   0.8852941176470588\ntruth_fscore   0.9226053639846743\nlie_precision   0.060240963855421686\nlie_recall   0.17857142857142858\nlie_fscore   0.09009009009009009\naccuracy   0.8573446327683616\nEpoch 4 Train Loss: 0.6197  Train F1: 0.511  Val Loss: 0.6431  Val Macro-F1: 0.5063  Val Lie-F1: 0.0901\nNo improvement for  3 epochs\nmacro_f1   0.46149534677944787\nmicro_f1   0.7168079096045198\ntruth_precision   0.965082444228904\ntruth_recall   0.7316176470588235\ntruth_fscore   0.8322877457130907\nlie_precision   0.05194805194805195\nlie_recall   0.35714285714285715\nlie_fscore   0.09070294784580499\naccuracy   0.7168079096045198\nEpoch 5 Train Loss: 0.5874  Train F1: 0.5528  Val Loss: 0.6404  Val Macro-F1: 0.4615  Val Lie-F1: 0.0907\nNo improvement for  4 epochs\nmacro_f1   0.488979608053478\nmicro_f1   0.9230225988700564\ntruth_precision   0.9595885378398237\ntruth_recall   0.9602941176470589\ntruth_fscore   0.959941198088938\nlie_precision   0.01818181818181818\nlie_recall   0.017857142857142856\nlie_fscore   0.018018018018018014\naccuracy   0.9230225988700564\nEpoch 6 Train Loss: 0.5498  Train F1: 0.5553  Val Loss: 0.8027  Val Macro-F1: 0.489  Val Lie-F1: 0.018\nNo improvement for  5 epochs\nEarly stopping triggered.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"model.load_state_dict(best_model_state[\"model_state\"])\ntest_loss, test_macro_f1, test_lie_f1, test_accuracy = evaluate(model, test_loader)\nprint(f\"Test Loss:         {test_loss:.4f}\")\nprint(f\"Test Macro-F1:     {test_macro_f1:.4f}\")\nprint(f\"Test Lie-F1:       {test_lie_f1:.4f}\")\nprint(f\"Test Accuracy:     {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:35:00.795782Z","iopub.execute_input":"2025-03-25T17:35:00.796087Z","iopub.status.idle":"2025-03-25T17:35:01.129365Z","shell.execute_reply.started":"2025-03-25T17:35:00.796063Z","shell.execute_reply":"2025-03-25T17:35:01.128556Z"}},"outputs":[{"name":"stdout","text":"macro_f1   0.5302637858475421\nmicro_f1   0.8847136081722\ntruth_precision   0.9168256390690576\ntruth_recall   0.9608156737305078\ntruth_fscore   0.938305349472862\nlie_precision   0.18333333333333332\nlie_recall   0.09166666666666666\nlie_fscore   0.12222222222222222\naccuracy   0.8847136081721999\nTest Loss:         1.1939\nTest Macro-F1:     0.5303\nTest Lie-F1:       0.1222\nTest Accuracy:     0.8847\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef evaluate(model, data_loader):\n    model.eval()\n    model.reset_metrics()\n    tot = 0.0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for x, y in data_loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            output = model(x, labels=y)\n            logits = output[\"logits\"]\n            loss = output[\"loss\"]\n\n            tot += loss.item() * y.size(0)\n\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.cpu().numpy())\n\n    av = tot / len(data_loader.dataset)\n    metrics = model.compute_metrics()\n    acc = accuracy_score(all_labels, all_preds)\n    metrics[\"accuracy\"] = acc\n\n    for k, v in metrics.items():\n        print(k, \" \", v)\n\n    return av, metrics[\"macro_f1\"], metrics[\"lie_fscore\"], metrics[\"accuracy\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:29:12.861995Z","iopub.execute_input":"2025-04-03T09:29:12.862296Z","iopub.status.idle":"2025-04-03T09:29:12.869627Z","shell.execute_reply.started":"2025-04-03T09:29:12.862272Z","shell.execute_reply":"2025-04-03T09:29:12.868553Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"torch.save(best_model_state[\"model_state\"], \"best_lie_detector_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:44:51.064867Z","iopub.execute_input":"2025-03-25T17:44:51.065208Z","iopub.status.idle":"2025-03-25T17:44:53.026138Z","shell.execute_reply.started":"2025-03-25T17:44:51.065184Z","shell.execute_reply":"2025-03-25T17:44:53.025432Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"model = LieDetector(\n    embedding_matrix=embedding_mat,\n    hidden_size=100,\n    use_power=False,\n    posclass_weight=30.0,\n    dropout=0.5,\n    num_classes=2\n).to(device)\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/deceptino_baseline_lstm/pytorch/default/1/best_lie_detector_model.pt\"))\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:29:15.680063Z","iopub.execute_input":"2025-04-03T09:29:15.680358Z","iopub.status.idle":"2025-04-03T09:29:16.921670Z","shell.execute_reply.started":"2025-04-03T09:29:15.680335Z","shell.execute_reply":"2025-04-03T09:29:16.920982Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"LieDetector(\n  (embedding): Embedding(1193515, 200, padding_idx=0)\n  (encoder): LSTM(200, 100, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (classifier): Linear(in_features=200, out_features=2, bias=True)\n  (loss_fn): CrossEntropyLoss()\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"evaluate(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:30:12.280488Z","iopub.execute_input":"2025-04-03T09:30:12.280818Z","iopub.status.idle":"2025-04-03T09:30:12.648530Z","shell.execute_reply.started":"2025-04-03T09:30:12.280796Z","shell.execute_reply":"2025-04-03T09:30:12.647664Z"}},"outputs":[{"name":"stdout","text":"macro_f1   0.5302637858475421\nmicro_f1   0.8847136081722\ntruth_precision   0.9168256390690576\ntruth_recall   0.9608156737305078\ntruth_fscore   0.938305349472862\nlie_precision   0.18333333333333332\nlie_recall   0.09166666666666666\nlie_fscore   0.12222222222222222\naccuracy   0.8847136081721999\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(1.193870038740046,\n 0.5302637858475421,\n 0.12222222222222222,\n 0.8847136081721999)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def glove_embed(glove_file, dim=200):\n    \n    w2i = {}\n    i2w = []\n    embeddings = []\n\n    w2i[\"<PAD>\"] = 0\n    i2w.append(\"<PAD>\")\n    embeddings.append(np.zeros(dim, dtype=np.float32))\n\n    w2i[\"<UNK>\"] = 1\n    i2w.append(\"<UNK>\")\n    unk_vector = np.random.normal(scale=0.06, size=(dim,)).astype(np.float32)\n    embeddings.append(unk_vector)\n\n    idx = 2  # real words start at index 2\n    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.strip().split()\n    \n            if len(values) != dim + 1:\n                continue\n            word = values[0]\n            vector = np.asarray(values[1:], dtype=np.float32)\n            w2i[word] = idx\n            i2w.append(word)\n            embeddings.append(vector)\n            idx += 1\n\n    embedding_matrix = torch.FloatTensor(np.array(embeddings))\n    return w2i, i2w, embedding_matrix\n\n\nglove_path = \"/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt\"\ndim = 200\nw2i, i2w, embed_mat = glove_embed(glove_path, dim)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm \n\ndef tokenize(text: str):\n    return text.lower().strip().split()\n\ndef text_to_ids(text: str, w2i: Dict[str, int]):\n    tokens = tokenize(text)\n    ids = []\n    for t in tokens:\n        if t in w2i:\n            ids.append(w2i[t])\n        else:\n            ids.append(w2i[\"<UNK>\"])  # <UNK>\n    return ids\n\n\nclass DiplomacyDataset(Dataset):\n    def __init__(self, file_path, w2i,fg=False, label_key=\"sender_labels\"):\n        self.samples = []\n        self.w2i = w2i\n        self.fg = fg\n        self.label_key = label_key\n\n        with open(file_path, 'r') as f:\n            for line in f:\n                convo = json.loads(line)\n                messages = convo[\"messages\"]\n                labels = convo[label_key]\n                # 'speakers' read here if needed\n                gs = convo.get(\"game_score_delta\", [0] * len(messages))\n\n                sample = self.preproc(messages, labels, gs)\n                if sample:\n                    self.samples.append(sample)\n\n        print(\"Loaded\", len(self.samples), \"conversations from\", file_path)\n\n    def preproc(self, messages, labels, gs):\n        token_ids_list = []\n        label_ids = []\n        score_deltas = []\n\n        for msg, label, g in zip(messages, labels, gs):\n        \n            if label not in [True, False, 'true', 'false', 'True', 'False']:  #checking for no annotaiton\n                continue\n\n            label_id = 1 if str(label).lower() == \"true\" else 0\n            token_ids = self.text_to_ids(msg)\n\n            # skip empty messages\n            if len(token_ids) == 0:\n                continue\n\n            token_ids_list.append(token_ids)\n            label_ids.append(label_id)\n            score_deltas.append(g)\n\n        if len(token_ids_list) == 0:\n            return None\n\n        return {\n            \"messages\": token_ids_list,    \n            \"labels\": label_ids,           \n            \"game_scores\": score_deltas if self.fg else None\n        }\n\n    def text_to_ids(self, text):\n        return [self.w2i.get(token, self.w2i[\"<UNK>\"]) for token in text.lower().strip().split()]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n\n\ndef pad(batch):\n    \n    b_msg = [item[\"messages\"] for item in batch]\n    b_lbl = [item[\"labels\"] for item in batch]\n    fgg = any(item[\"game_scores\"] is not None for item in batch)\n\n\n    mxt = max(len(convo) for convo in b_msg)\n    mxl = max(len(msg) for convo in b_msg for msg in convo)\n\n    ip_pad = []\n    lbl_pad = []\n    scores_pad = []\n    mask = []\n\n    for i in range(len(b_msg)):\n        conv = b_msg[i]\n        labels = b_lbl[i]\n        scores = batch[i][\"game_scores\"] if batch[i][\"game_scores\"] is not None else None\n\n        conv_padded = []\n        label_padded = []\n        score_padded = []\n        mask_row = []\n\n        for j in range(mxt):\n            if j < len(conv):\n                msg = conv[j]\n                tmp = msg + [0] * (mxl - len(msg))\n                conv_padded.append(tmp)\n                label_padded.append(labels[j])\n                mask_row.append(1)\n                if scores is not None:\n                    score_padded.append(scores[j])\n            else:\n                conv_padded.append([0] * mxl)\n                label_padded.append(-100)\n                mask_row.append(0)\n                if scores is not None:\n                    score_padded.append(0)\n\n        ip_pad.append(conv_padded)\n        lbl_pad.append(label_padded)\n        if fgg:\n            scores_pad.append(score_padded)\n        mask.append(mask_row)\n\n    iid = torch.LongTensor(ip_pad)  \n    lbls = torch.LongTensor(lbl_pad)     \n    gs = torch.FloatTensor(scores_pad) if fgg else None\n    mask = torch.BoolTensor(mask)               \n\n    return iid, lbls, gs, mask\n\n\ntrain_path = \"/kaggle/input/deception/train.jsonl\"\nval_path   = \"/kaggle/input/deception/validation.jsonl\"\ntest_path  = \"/kaggle/input/deception/test.jsonl\"\n\ntrain_dataset = DiplomacyDataset(\n    file_path=train_path,\n    w2i=w2i,\n    fg=False,    \n    label_key=\"sender_labels\"\n)\nval_dataset = DiplomacyDataset(\n    file_path=val_path,\n    w2i=w2i,\n    fg=False,\n    label_key=\"sender_labels\"\n)\ntest_dataset = DiplomacyDataset(\n    file_path=test_path,\n    w2i=w2i,\n    fg=False,\n    label_key=\"sender_labels\"\n)\n\nbatch_size = 4  # from config\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=pad\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=pad\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=pad\n)\n\n\nclass HierarchicalLSTM(nn.Module):\n    def __init__(self,embedding_matrix,message_hidden_size=100,convo_hidden_size=200,dropout=0.3,pos_weight=10.0,num_classes=2):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n        emb_dim = embedding_matrix.shape[1]\n        self.msg_enc = nn.LSTM(input_size=emb_dim, hidden_size=message_hidden_size, batch_first=True, bidirectional=True)\n        self.conv_enc = nn.LSTM(input_size=2 * message_hidden_size, hidden_size=convo_hidden_size, batch_first=True, bidirectional=False)\n        self.mlp = nn.Linear(convo_hidden_size, num_classes)\n        self.dropout = nn.Dropout(dropout)\n        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([pos_weight, 1.0]))\n\n\n        self.reset_metrics()\n\n    def reset_metrics(self):\n        self.true_y = []\n        self.pred_y = []\n\n    def compute_metrics(self):\n        if len(self.true_y) == 0:\n            return {}\n    \n        y_true = torch.cat(self.true_y).cpu().numpy()\n        y_pred = torch.cat(self.pred_y).cpu().numpy()\n    \n        macro_f1 = f1_score(y_true, y_pred, average='macro')\n        micro_f1 = f1_score(y_true, y_pred, average='micro')\n        lie_f1   = f1_score(y_true, y_pred, pos_label=0)\n        acc      = (y_true == y_pred).mean()\n    \n        return {\n            \"macro_f1\": macro_f1,\n            \"micro_f1\": micro_f1,\n            \"lie_f1\": lie_f1,\n            \"accuracy\": acc,\n            \"lie_precision\": precision_score(y_true, y_pred, pos_label=0),\n            \"lie_recall\": recall_score(y_true, y_pred, pos_label=0),\n            \"truth_precision\": precision_score(y_true, y_pred, pos_label=1),\n            \"truth_recall\": recall_score(y_true, y_pred, pos_label=1),\n        }\n\n\n    def forward(self, input_ids, labels=None, game_scores=None, mask=None):\n\n        b, t, l = input_ids.shape\n\n        fids = input_ids.view(b * t, l)\n\n        mask_seq = (fids != 0)\n        lns  = mask_seq.sum(dim=1)\n        valid = (lns > 0).nonzero(as_tuple=True)[0]\n\n        fids = fids[valid]\n        mask_seq= mask_seq[valid]\n        lns= lns[valid]\n\n        tmp_embd = self.embedding(fids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            tmp_embd, lns.cpu(), batch_first=True, enforce_sorted=False\n        )\n        pk_out, tmp = self.msg_enc(packed)\n        enc_seq, tmp  = nn.utils.rnn.pad_packed_sequence(pk_out, batch_first=True)\n\n      \n        D = enc_seq.size(-1) \n        mask_expanded = mask_seq.unsqueeze(-1).expand(-1, -1, D)\n        enc_seq = enc_seq.masked_fill(~mask_expanded, float('-inf'))\n        pooled = torch.max(enc_seq, dim=1)[0] \n\n        pool = torch.zeros(b * t, 2 * self.msg_enc.hidden_size,\n                                      device=pooled.device, dtype=pooled.dtype)\n        pool[valid] = pooled\n        pool = pool.view(b, t, -1)\n\n        msg_ct = mask.sum(dim=1) \n        pck   = nn.utils.rnn.pack_padded_sequence(pool, msg_ct.cpu(), batch_first=True, enforce_sorted=False )\n        cn_out, tmp  = self.conv_enc(pck)\n        econv, tmp  = nn.utils.rnn.pad_packed_sequence(cn_out, batch_first=True)\n\n      \n        econv = self.dropout(econv)\n        logits = self.mlp(econv) \n\n        output = {\"logits\": logits}\n\n        if labels is not None:\n            loss = self.masked_loss(logits, labels, mask)\n            output[\"loss\"] = loss\n\n            preds = torch.argmax(logits, dim=-1)\n            lbls_v = labels[mask]\n            pred_v  = preds[mask]\n            self.true_y.append(lbls_v.detach())\n            self.pred_y.append(pred_v.detach())\n\n        return output\n\n    def masked_loss(self, logits, labels, mask):\n        lg2d = logits.view(-1, logits.size(-1))      \n        lg1d = labels.view(-1)                       \n        msk1d   = mask.view(-1).float()                \n        ptl = self.loss_fn(lg2d, lg1d)\n        masked_loss    = (ptl * msk1d).sum() / (msk1d.sum() + 1e-8)\n        return masked_loss\n\nmodel = HierarchicalLSTM(embed_mat.to(device), 100, 200, 0.3, 10.0, 2).to(device)\n\n\noptimizer, lr, num_epochs, patience, grad_clip = optim.Adam(model.parameters(), lr=0.003), 0.003, 15, 10, 1.0\n\nb_f1 = -1\nep_no = 0\nbest_model_state = None\n\ndef evaluate(model, data_loader):\n    model.eval()\n    model.reset_metrics()\n    tot = 0.0\n    ct = 0\n\n    with torch.no_grad():\n        for iids, labels, game_scores, mask in data_loader:\n            iids = iids.to(device)\n            labels= labels.to(device)\n            mask= mask.to(device)\n            if game_scores is not None:\n                game_scores = game_scores.to(device)\n\n            output = model(iids, labels=labels, game_scores=game_scores, mask=mask)\n\n            sz = iids.size(0)\n            tot  += output[\"loss\"].item() * sz\n            ct += sz\n\n    av = tot / (ct if ct > 0 else 1)\n    m  = model.compute_metrics()\n    for k, v in m.items():\n        print(k,\"  \",v)\n\n    return av, m.get(\"macro_f1\", 0.0), m.get(\"lie_f1\", 0.0), m.get(\"accuracy\", 0.0)\n\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    model.reset_metrics()\n    ttl = 0.0\n    ct_tr = 0\n\n    for iids, labels, game_scores, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        iids = iids.to(device)\n        labels    = labels.to(device)\n        mask      = mask.to(device)\n        if game_scores is not None:\n            game_scores = game_scores.to(device)\n\n        optimizer.zero_grad()\n        output = model(iids, labels=labels, game_scores=game_scores, mask=mask)\n        loss = output[\"loss\"]\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n\n        sz = iids.size(0)\n        ttl += loss.item() * sz\n        ct_tr      += sz\n\n    av_tl = ttl / (ct_tr if ct_tr>0 else 1)\n    tmr  = model.compute_metrics()\n    t_f1      = tmr.get(\"macro_f1\", 0.0)\n\n    vl, val_f1, lie_f1, acc = evaluate(model, val_loader)\n    print(\"Epoch \", epoch + 1, \"\", num_epochs, \" Train Loss:\", round(av_tl, 4), \" Train Macro-F1:\", round(t_f1, 4), \" Val Loss:\", round(vl, 4), \" Val Macro-F1:\", round(val_f1, 4), \"| Lie F1:\", round(lie_f1, 4), \" Val Accuracy:\", round(acc, 4))\n\n    if val_f1 > b_f1:\n        b_f1 = val_f1\n        ep_no = 0\n        best_model_state = {\n            \"epoch\": epoch + 1,\n            \"model_state\": model.state_dict(),\n            \"val_f1\": val_f1\n        }\n        print(f\" New best model saved (Val F1: {val_f1:.4f})\")\n    else:\n        ep_no += 1\n        print(f\"  No improvement for {ep_no} epoch\")\n        if ep_no >= patience:\n            print(\" Early stopping triggered.\")\n            break\n\nprint()\nprint()\nprint(\"BEST MODEL\")\nmodel.load_state_dict(best_model_state[\"model_state\"])\ntest_loss, test_f1, tl_prec, tl_rec = evaluate(model, test_loader)\nprint(\"test set results\")\nprint( \"Test Loss:\", round(test_loss, 4), \"Test Macro-F1:\", round(test_f1, 4), \"Lie Precision:\", round(tl_prec, 4), \"Lie Recall:\", round(tl_rec, 4))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:10:25.421733Z","iopub.execute_input":"2025-03-25T18:10:25.422030Z","iopub.status.idle":"2025-03-25T18:11:13.926665Z","shell.execute_reply.started":"2025-03-25T18:10:25.422006Z","shell.execute_reply":"2025-03-25T18:11:13.925576Z"}},"outputs":[{"name":"stdout","text":"Loaded 184 conversations from /kaggle/input/deception/train.jsonl\nLoaded 20 conversations from /kaggle/input/deception/validation.jsonl\nLoaded 42 conversations from /kaggle/input/deception/test.jsonl\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 46/46 [00:03<00:00, 14.47it/s]\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.4899135446685879\nmicro_f1    0.96045197740113\nlie_f1    0.0\naccuracy    0.96045197740113\nlie_precision    0.0\nlie_recall    0.0\ntruth_precision    0.96045197740113\ntruth_recall    1.0\nEpoch  1  15  Train Loss: 0.653  Train Macro-F1: 0.4924  Val Loss: 0.6162  Val Macro-F1: 0.4899 | Lie F1: 0.0  Val Accuracy: 0.9605\n New best model saved (Val F1: 0.4899)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 46/46 [00:02<00:00, 15.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.4897297297297297\nmicro_f1    0.9597457627118644\nlie_f1    0.0\naccuracy    0.9597457627118644\nlie_precision    0.0\nlie_recall    0.0\ntruth_precision    0.9604240282685512\ntruth_recall    0.9992647058823529\nEpoch  2  15  Train Loss: 0.6329  Train Macro-F1: 0.4884  Val Loss: 0.5953  Val Macro-F1: 0.4897 | Lie F1: 0.0  Val Accuracy: 0.9597\n  No improvement for 1 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 46/46 [00:02<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.50624387054593\nmicro_f1    0.864406779661017\nlie_f1    0.08571428571428572\naccuracy    0.864406779661017\nlie_precision    0.05844155844155844\nlie_recall    0.16071428571428573\ntruth_precision    0.9627575277337559\ntruth_recall    0.8933823529411765\nEpoch  3  15  Train Loss: 0.5985  Train Macro-F1: 0.514  Val Loss: 0.6107  Val Macro-F1: 0.5062 | Lie F1: 0.0857  Val Accuracy: 0.8644\n New best model saved (Val F1: 0.5062)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 46/46 [00:03<00:00, 15.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5031713044182343\nmicro_f1    0.9540960451977402\nlie_f1    0.029850746268656716\naccuracy    0.9540960451977402\nlie_precision    0.09090909090909091\nlie_recall    0.017857142857142856\ntruth_precision    0.9608540925266904\ntruth_recall    0.9926470588235294\nEpoch  4  15  Train Loss: 0.5897  Train Macro-F1: 0.5554  Val Loss: 0.6132  Val Macro-F1: 0.5032 | Lie F1: 0.0299  Val Accuracy: 0.9541\n  No improvement for 1 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 46/46 [00:02<00:00, 15.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5058457629717239\nmicro_f1    0.8312146892655368\nlie_f1    0.10486891385767791\naccuracy    0.8312146892655368\nlie_precision    0.06635071090047394\nlie_recall    0.25\ntruth_precision    0.9651452282157676\ntruth_recall    0.8551470588235294\nEpoch  5  15  Train Loss: 0.5348  Train Macro-F1: 0.5492  Val Loss: 0.5799  Val Macro-F1: 0.5058 | Lie F1: 0.1049  Val Accuracy: 0.8312\n  No improvement for 2 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 46/46 [00:03<00:00, 15.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.49383771350418854\nmicro_f1    0.9350282485875706\nlie_f1    0.02127659574468085\naccuracy    0.9350282485875706\nlie_precision    0.02631578947368421\nlie_recall    0.017857142857142856\ntruth_precision    0.9600870827285921\ntruth_recall    0.9727941176470588\nEpoch  6  15  Train Loss: 0.4561  Train Macro-F1: 0.6041  Val Loss: 0.9726  Val Macro-F1: 0.4938 | Lie F1: 0.0213  Val Accuracy: 0.935\n  No improvement for 3 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 46/46 [00:02<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5368923335380359\nmicro_f1    0.9018361581920904\nlie_f1    0.12578616352201258\naccuracy    0.9018361581920904\nlie_precision    0.0970873786407767\nlie_recall    0.17857142857142858\ntruth_precision    0.964965727341965\ntruth_recall    0.9316176470588236\nEpoch  7  15  Train Loss: 0.3573  Train Macro-F1: 0.6345  Val Loss: 0.8033  Val Macro-F1: 0.5369 | Lie F1: 0.1258  Val Accuracy: 0.9018\n New best model saved (Val F1: 0.5369)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15: 100%|██████████| 46/46 [00:02<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5139727977392516\nmicro_f1    0.8848870056497176\nlie_f1    0.08938547486033518\naccuracy    0.8848870056497176\nlie_precision    0.06504065040650407\nlie_recall    0.14285714285714285\ntruth_precision    0.962877030162413\ntruth_recall    0.9154411764705882\nEpoch  8  15  Train Loss: 0.305  Train Macro-F1: 0.686  Val Loss: 0.8721  Val Macro-F1: 0.514 | Lie F1: 0.0894  Val Accuracy: 0.8849\n  No improvement for 1 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15: 100%|██████████| 46/46 [00:03<00:00, 14.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.49655688622754496\nmicro_f1    0.8926553672316384\nlie_f1    0.05\naccuracy    0.8926553672316384\nlie_precision    0.038461538461538464\nlie_recall    0.07142857142857142\ntruth_precision    0.9603658536585366\ntruth_recall    0.9264705882352942\nEpoch  9  15  Train Loss: 0.1809  Train Macro-F1: 0.7631  Val Loss: 1.1839  Val Macro-F1: 0.4966 | Lie F1: 0.05  Val Accuracy: 0.8927\n  No improvement for 2 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15: 100%|██████████| 46/46 [00:03<00:00, 14.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.487698986975398\nmicro_f1    0.9519774011299436\nlie_f1    0.0\naccuracy    0.9519774011299436\nlie_precision    0.0\nlie_recall    0.0\ntruth_precision    0.9601139601139601\ntruth_recall    0.9911764705882353\nEpoch  10  15  Train Loss: 0.1266  Train Macro-F1: 0.8201  Val Loss: 1.87  Val Macro-F1: 0.4877 | Lie F1: 0.0  Val Accuracy: 0.952\n  No improvement for 3 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15: 100%|██████████| 46/46 [00:03<00:00, 13.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5024304538799413\nmicro_f1    0.9322033898305084\nlie_f1    0.04\naccuracy    0.9322033898305084\nlie_precision    0.045454545454545456\nlie_recall    0.03571428571428571\ntruth_precision    0.9606413994169096\ntruth_recall    0.9691176470588235\nEpoch  11  15  Train Loss: 0.0928  Train Macro-F1: 0.875  Val Loss: 1.5952  Val Macro-F1: 0.5024 | Lie F1: 0.04  Val Accuracy: 0.9322\n  No improvement for 4 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15: 100%|██████████| 46/46 [00:03<00:00, 14.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5192738361843046\nmicro_f1    0.9194915254237288\nlie_f1    0.08064516129032258\naccuracy    0.9194915254237288\nlie_precision    0.07352941176470588\nlie_recall    0.08928571428571429\ntruth_precision    0.962166172106825\ntruth_recall    0.9536764705882353\nEpoch  12  15  Train Loss: 0.0546  Train Macro-F1: 0.9186  Val Loss: 1.57  Val Macro-F1: 0.5193 | Lie F1: 0.0806  Val Accuracy: 0.9195\n  No improvement for 5 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15: 100%|██████████| 46/46 [00:03<00:00, 15.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.512796486090776\nmicro_f1    0.9336158192090396\nlie_f1    0.05999999999999999\naccuracy    0.9336158192090396\nlie_precision    0.06818181818181818\nlie_recall    0.05357142857142857\ntruth_precision    0.9613702623906706\ntruth_recall    0.9698529411764706\nEpoch  13  15  Train Loss: 0.0382  Train Macro-F1: 0.9145  Val Loss: 2.1315  Val Macro-F1: 0.5128 | Lie F1: 0.06  Val Accuracy: 0.9336\n  No improvement for 6 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15: 100%|██████████| 46/46 [00:03<00:00, 15.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5052585709406541\nmicro_f1    0.9371468926553672\nlie_f1    0.043010752688172046\naccuracy    0.9371468926553672\nlie_precision    0.05405405405405406\nlie_recall    0.03571428571428571\ntruth_precision    0.9608411892675852\ntruth_recall    0.9742647058823529\nEpoch  14  15  Train Loss: 0.0387  Train Macro-F1: 0.9293  Val Loss: 2.0601  Val Macro-F1: 0.5053 | Lie F1: 0.043  Val Accuracy: 0.9371\n  No improvement for 7 epoch\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15: 100%|██████████| 46/46 [00:03<00:00, 14.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.49662282260931395\nmicro_f1    0.9209039548022597\nlie_f1    0.03448275862068965\naccuracy    0.9209039548022598\nlie_precision    0.03333333333333333\nlie_recall    0.03571428571428571\ntruth_precision    0.9601769911504425\ntruth_recall    0.9573529411764706\nEpoch  15  15  Train Loss: 0.0269  Train Macro-F1: 0.9459  Val Loss: 2.0281  Val Macro-F1: 0.4966 | Lie F1: 0.0345  Val Accuracy: 0.9209\n  No improvement for 8 epoch\n\n\nBEST MODEL\nmacro_f1    0.5351159690961913\nmicro_f1    0.8759576796789493\nlie_f1    0.13705583756345177\naccuracy    0.8759576796789493\nlie_precision    0.17532467532467533\nlie_recall    0.1125\ntruth_precision    0.9176652493235408\ntruth_recall    0.9492203118752499\ntest set results\nTest Loss: 3.2413 Test Macro-F1: 0.5351 Lie Precision: 0.1371 Lie Recall: 0.876\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"torch.save(best_model_state[\"model_state\"], \"best_context_lstm.pt\")\nmodel = HierarchicalLSTM(\n    embedding_matrix=embed_mat.to(device),\n    message_hidden_size=100,\n    convo_hidden_size=200,\n    dropout=0.3,\n    pos_weight=10.0,\n    num_classes=2\n).to(device)\n\nmodel.load_state_dict(torch.load(\"best_context_lstm.pt\"))\nmodel.eval()\n\ntest_loss, test_f1, tl_prec, tl_rec = evaluate(model, test_loader)\nprint(\"test set results\")\nprint( \"Test Loss:\", round(test_loss, 4), \"Test Macro-F1:\", round(test_f1, 4), \"Lie Precision:\", round(tl_prec, 4), \"Lie Recall:\", round(tl_rec, 4))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:16:12.974227Z","iopub.execute_input":"2025-03-25T18:16:12.974617Z","iopub.status.idle":"2025-03-25T18:16:16.363242Z","shell.execute_reply.started":"2025-03-25T18:16:12.974585Z","shell.execute_reply":"2025-03-25T18:16:16.362267Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-60-7f8e90d5266c>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_context_lstm.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"macro_f1    0.5351159690961913\nmicro_f1    0.8759576796789493\nlie_f1    0.13705583756345177\naccuracy    0.8759576796789493\nlie_precision    0.17532467532467533\nlie_recall    0.1125\ntruth_precision    0.9176652493235408\ntruth_recall    0.9492203118752499\ntest set results\nTest Loss: 3.2413 Test Macro-F1: 0.5351 Lie Precision: 0.1371 Lie Recall: 0.876\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"def tokenize(text: str):\n    return text.lower().strip().split()\n\ndef text_to_ids(text: str, w2i: Dict[str, int]):\n    tokens = tokenize(text)\n    ids = []\n    for t in tokens:\n        if t in w2i:\n            ids.append(w2i[t])\n        else:\n            ids.append(w2i[\"<UNK>\"])  # <UNK>\n    return ids\n\n\nclass DiplomacyDataset(Dataset):\n    def __init__(self, file_path, w2i,fg=False, label_key=\"sender_labels\"):\n        self.samples = []\n        self.w2i = w2i\n        self.fg = fg\n        self.label_key = label_key\n\n        with open(file_path, 'r') as f:\n            for line in f:\n                convo = json.loads(line)\n                messages = convo[\"messages\"]\n                labels = convo[label_key]\n                # 'speakers' read here if needed\n                gs = convo.get(\"game_score_delta\", [0] * len(messages))\n\n                sample = self.preproc(messages, labels, gs)\n                if sample:\n                    self.samples.append(sample)\n\n        print(\"Loaded\", len(self.samples), \"conversations from\", file_path)\n\n    def preproc(self, messages, labels, gs):\n        token_ids_list = []\n        label_ids = []\n        score_deltas = []\n\n        for msg, label, g in zip(messages, labels, gs):\n        \n            if label not in [True, False, 'true', 'false', 'True', 'False']:  #checking for no annotaiton\n                continue\n\n            label_id = 1 if str(label).lower() == \"true\" else 0\n            token_ids = self.text_to_ids(msg)\n\n            # skip empty messages\n            if len(token_ids) == 0:\n                continue\n\n            token_ids_list.append(token_ids)\n            label_ids.append(label_id)\n            score_deltas.append(g)\n\n        if len(token_ids_list) == 0:\n            return None\n\n        return {\n            \"messages\": token_ids_list,    \n            \"labels\": label_ids,           \n            \"game_scores\": score_deltas if self.fg else None\n        }\n\n    def text_to_ids(self, text):\n        return [self.w2i.get(token, self.w2i[\"<UNK>\"]) for token in text.lower().strip().split()]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n\n\ndef pad(batch):\n    \n    b_msg = [item[\"messages\"] for item in batch]\n    b_lbl = [item[\"labels\"] for item in batch]\n    fgg = any(item[\"game_scores\"] is not None for item in batch)\n\n\n    mxt = max(len(convo) for convo in b_msg)\n    mxl = max(len(msg) for convo in b_msg for msg in convo)\n\n    ip_pad = []\n    lbl_pad = []\n    scores_pad = []\n    mask = []\n\n    for i in range(len(b_msg)):\n        conv = b_msg[i]\n        labels = b_lbl[i]\n        scores = batch[i][\"game_scores\"] if batch[i][\"game_scores\"] is not None else None\n\n        conv_padded = []\n        label_padded = []\n        score_padded = []\n        mask_row = []\n\n        for j in range(mxt):\n            if j < len(conv):\n                msg = conv[j]\n                tmp = msg + [0] * (mxl - len(msg))\n                conv_padded.append(tmp)\n                label_padded.append(labels[j])\n                mask_row.append(1)\n                if scores is not None:\n                    score_padded.append(scores[j])\n            else:\n                conv_padded.append([0] * mxl)\n                label_padded.append(-100)\n                mask_row.append(0)\n                if scores is not None:\n                    score_padded.append(0)\n\n        ip_pad.append(conv_padded)\n        lbl_pad.append(label_padded)\n        if fgg:\n            scores_pad.append(score_padded)\n        mask.append(mask_row)\n\n    iid = torch.LongTensor(ip_pad)  \n    lbls = torch.LongTensor(lbl_pad)     \n    gs = torch.FloatTensor(scores_pad) if fgg else None\n    mask = torch.BoolTensor(mask)               \n\n    return iid, lbls, gs, mask\n\n\ntrain_path = \"/kaggle/input/deception/train.jsonl\"\nval_path   = \"/kaggle/input/deception/validation.jsonl\"\ntest_path  = \"/kaggle/input/deception/test.jsonl\"\n\ntrain_dataset = DiplomacyDataset(\n    file_path=train_path,\n    w2i=w2i,\n    fg=False,    \n    label_key=\"sender_labels\"\n)\nval_dataset = DiplomacyDataset(\n    file_path=val_path,\n    w2i=w2i,\n    fg=False,\n    label_key=\"sender_labels\"\n)\ntest_dataset = DiplomacyDataset(\n    file_path=test_path,\n    w2i=w2i,\n    fg=False,\n    label_key=\"sender_labels\"\n)\n\nbatch_size = 4  # from config\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=pad\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=pad\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=pad\n)\n\n\nclass HierarchicalLSTM(nn.Module):\n    def __init__(self,embedding_matrix,message_hidden_size=100,convo_hidden_size=200,dropout=0.3,pos_weight=10.0,num_classes=2):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n        emb_dim = embedding_matrix.shape[1]\n        self.msg_enc = nn.LSTM(input_size=emb_dim, hidden_size=message_hidden_size, batch_first=True, bidirectional=True)\n        self.conv_enc = nn.LSTM(input_size=2 * message_hidden_size, hidden_size=convo_hidden_size, batch_first=True, bidirectional=False)\n        self.mlp = nn.Linear(convo_hidden_size, num_classes)\n        self.dropout = nn.Dropout(dropout)\n        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([pos_weight, 1.0]))\n\n\n        self.reset_metrics()\n\n    def reset_metrics(self):\n        self.true_y = []\n        self.pred_y = []\n\n    def compute_metrics(self):\n        if len(self.true_y) == 0:\n            return {}\n    \n        y_true = torch.cat(self.true_y).cpu().numpy()\n        y_pred = torch.cat(self.pred_y).cpu().numpy()\n    \n        macro_f1 = f1_score(y_true, y_pred, average='macro')\n        micro_f1 = f1_score(y_true, y_pred, average='micro')\n        lie_f1   = f1_score(y_true, y_pred, pos_label=0)\n        acc      = (y_true == y_pred).mean()\n    \n        return {\n            \"macro_f1\": macro_f1,\n            \"micro_f1\": micro_f1,\n            \"lie_f1\": lie_f1,\n            \"accuracy\": acc,\n            \"lie_precision\": precision_score(y_true, y_pred, pos_label=0),\n            \"lie_recall\": recall_score(y_true, y_pred, pos_label=0),\n            \"truth_precision\": precision_score(y_true, y_pred, pos_label=1),\n            \"truth_recall\": recall_score(y_true, y_pred, pos_label=1),\n        }\n\n\n    def forward(self, input_ids, labels=None, game_scores=None, mask=None):\n\n        b, t, l = input_ids.shape\n\n        fids = input_ids.view(b * t, l)\n\n        mask_seq = (fids != 0)\n        lns  = mask_seq.sum(dim=1)\n        valid = (lns > 0).nonzero(as_tuple=True)[0]\n\n        fids = fids[valid]\n        mask_seq= mask_seq[valid]\n        lns= lns[valid]\n\n        tmp_embd = self.embedding(fids)\n        packed = nn.utils.rnn.pack_padded_sequence(\n            tmp_embd, lns.cpu(), batch_first=True, enforce_sorted=False\n        )\n        pk_out, tmp = self.msg_enc(packed)\n        enc_seq, tmp  = nn.utils.rnn.pad_packed_sequence(pk_out, batch_first=True)\n\n      \n        D = enc_seq.size(-1) \n        mask_expanded = mask_seq.unsqueeze(-1).expand(-1, -1, D)\n        enc_seq = enc_seq.masked_fill(~mask_expanded, float('-inf'))\n        pooled = torch.max(enc_seq, dim=1)[0] \n\n        pool = torch.zeros(b * t, 2 * self.msg_enc.hidden_size,\n                                      device=pooled.device, dtype=pooled.dtype)\n        pool[valid] = pooled\n        pool = pool.view(b, t, -1)\n\n        msg_ct = mask.sum(dim=1) \n        pck   = nn.utils.rnn.pack_padded_sequence(pool, msg_ct.cpu(), batch_first=True, enforce_sorted=False )\n        cn_out, tmp  = self.conv_enc(pck)\n        econv, tmp  = nn.utils.rnn.pad_packed_sequence(cn_out, batch_first=True)\n\n      \n        econv = self.dropout(econv)\n        logits = self.mlp(econv) \n\n        output = {\"logits\": logits}\n\n        if labels is not None:\n            loss = self.masked_loss(logits, labels, mask)\n            output[\"loss\"] = loss\n\n            preds = torch.argmax(logits, dim=-1)\n            lbls_v = labels[mask]\n            pred_v  = preds[mask]\n            self.true_y.append(lbls_v.detach())\n            self.pred_y.append(pred_v.detach())\n\n        return output\n\n    def masked_loss(self, logits, labels, mask):\n        lg2d = logits.view(-1, logits.size(-1))      \n        lg1d = labels.view(-1)                       \n        msk1d   = mask.view(-1).float()                \n        ptl = self.loss_fn(lg2d, lg1d)\n        masked_loss    = (ptl * msk1d).sum() / (msk1d.sum() + 1e-8)\n        return masked_loss\n\n\ndef evaluate(model, data_loader):\n    model.eval()\n    model.reset_metrics()\n    tot = 0.0\n    ct = 0\n\n    with torch.no_grad():\n        for iids, labels, game_scores, mask in data_loader:\n            iids = iids.to(device)\n            labels= labels.to(device)\n            mask= mask.to(device)\n            if game_scores is not None:\n                game_scores = game_scores.to(device)\n\n            output = model(iids, labels=labels, game_scores=game_scores, mask=mask)\n\n            sz = iids.size(0)\n            tot  += output[\"loss\"].item() * sz\n            ct += sz\n\n    av = tot / (ct if ct > 0 else 1)\n    m  = model.compute_metrics()\n    for k, v in m.items():\n        print(k,\"  \",v)\n\n    return av, m.get(\"macro_f1\", 0.0), m.get(\"lie_f1\", 0.0), m.get(\"accuracy\", 0.0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:32:37.887981Z","iopub.execute_input":"2025-04-03T09:32:37.888334Z","iopub.status.idle":"2025-04-03T09:32:38.072066Z","shell.execute_reply.started":"2025-04-03T09:32:37.888311Z","shell.execute_reply":"2025-04-03T09:32:38.071361Z"}},"outputs":[{"name":"stdout","text":"Loaded 184 conversations from /kaggle/input/deception/train.jsonl\nLoaded 20 conversations from /kaggle/input/deception/validation.jsonl\nLoaded 42 conversations from /kaggle/input/deception/test.jsonl\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model = HierarchicalLSTM(\n    embedding_matrix=embedding_mat.to(device),\n    message_hidden_size=100,\n    convo_hidden_size=200,\n    dropout=0.3,\n    pos_weight=10.0,\n    num_classes=2\n).to(device)\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/deception_baseline_context_lstm/pytorch/default/1/best_context_lstm.pt\"))\nmodel.eval()\n\ntest_loss, test_f1, tl_prec, tl_rec = evaluate(model, test_loader)\nprint(\"test set results\")\nprint( \"Test Loss:\", round(test_loss, 4), \"Test Macro-F1:\", round(test_f1, 4), \"Lie Precision:\", round(tl_prec, 4), \"Lie Recall:\", round(tl_rec, 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:33:56.947142Z","iopub.execute_input":"2025-04-03T09:33:56.947493Z","iopub.status.idle":"2025-04-03T09:34:05.277247Z","shell.execute_reply.started":"2025-04-03T09:33:56.947471Z","shell.execute_reply":"2025-04-03T09:34:05.276539Z"}},"outputs":[{"name":"stdout","text":"macro_f1    0.5351159690961913\nmicro_f1    0.8759576796789493\nlie_f1    0.13705583756345177\naccuracy    0.8759576796789493\nlie_precision    0.17532467532467533\nlie_recall    0.1125\ntruth_precision    0.9176652493235408\ntruth_recall    0.9492203118752499\ntest set results\nTest Loss: 3.2413 Test Macro-F1: 0.5351 Lie Precision: 0.1371 Lie Recall: 0.876\n","output_type":"stream"}],"execution_count":26}]}